import os
import json
import frontmatter
import markdown
from bs4 import BeautifulSoup
from datetime import datetime

# [ì¤‘ìš”] ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ê°€ script/ í´ë”ì´ë¯€ë¡œ, í”„ë¡œì íŠ¸ ë£¨íŠ¸ëŠ” ë¶€ëª¨ì˜ ë¶€ëª¨ í´ë”ì…ë‹ˆë‹¤.
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(SCRIPT_DIR)  # jinjaMap/

CONTENT_DIR = os.path.join(BASE_DIR, 'app', 'content')
STATIC_DIR = os.path.join(BASE_DIR, 'app', 'static')

JSON_OUTPUT = os.path.join(STATIC_DIR, 'json', 'shrines_data.json')
SITEMAP_OUTPUT = os.path.join(STATIC_DIR, 'sitemap.xml')
ADS_OUTPUT = os.path.join(STATIC_DIR, 'ads.txt')
BASE_URL = 'https://jinjamap.com'

def strip_markdown(text):
    try:
        html = markdown.markdown(text)
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text()
    except Exception as e:
        return text

def generate_sitemap(shrines):
    xml = ['<?xml version="1.0" encoding="UTF-8"?>']
    xml.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    
    last_updated = datetime.now().strftime("%Y-%m-%d")

    # ë©”ì¸ í˜ì´ì§€
    xml.append('  <url>')
    xml.append(f'    <loc>{BASE_URL}/</loc>')
    xml.append(f'    <lastmod>{last_updated}</lastmod>')
    xml.append('    <changefreq>daily</changefreq>')
    xml.append('    <priority>1.0</priority>')
    xml.append('  </url>')

    # ìƒì„¸ í˜ì´ì§€
    for shrine in shrines:
        link = shrine['link']
        date_str = shrine.get('published', last_updated)
        xml.append('  <url>')
        xml.append(f'    <loc>{BASE_URL}{link}</loc>')
        xml.append(f'    <lastmod>{date_str}</lastmod>')
        xml.append('    <changefreq>weekly</changefreq>')
        xml.append('    <priority>0.8</priority>')
        xml.append('  </url>')
        
    xml.append('</urlset>')
    return '\n'.join(xml)

def main():
    print(f"ğŸ”¨ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (Root: {BASE_DIR})")
    
    shrines = []
    
    os.makedirs(os.path.dirname(JSON_OUTPUT), exist_ok=True)
    if not os.path.exists(CONTENT_DIR):
        print(f"âŒ Content directory not found: {CONTENT_DIR}")
        return

    for filename in os.listdir(CONTENT_DIR):
        if not filename.endswith('.md'): continue
        
        filepath = os.path.join(CONTENT_DIR, filename)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
                
                # Draft ì²´í¬
                if post.get('draft') == True and not os.environ.get('DEV_MODE'): continue
                if not post.get('lat') or not post.get('lng'): continue
                
                date_val = post.get('date')
                published_date = str(date_val) if date_val else datetime.now().strftime('%Y-%m-%d')
                
                summary = post.get('summary')
                if not summary:
                    summary = strip_markdown(post.content)[:120] + '...'
                
                has_onsen = "Relax at a Nearby Onsen" in str(post.content)

                shrine = {
                    "id": filename.replace('.md', ''),
                    "title": post.get('title', 'No Title'),
                    "lat": post.get('lat'),
                    "lng": post.get('lng'),
                    "categories": post.get('categories', []),
                    "thumbnail": post.get('thumbnail', '/static/images/default.png'),
                    "address": post.get('address', ''),
                    "published": published_date,
                    "summary": summary,
                    "link": f"/shrine/{filename.replace('.md', '')}",
                    "has_onsen": has_onsen
                }
                shrines.append(shrine)
        except Exception as e:
            print(f"âŒ Error processing {filename}: {e}")

    # ìµœì‹ ìˆœ ì •ë ¬
    shrines.sort(key=lambda x: x['published'], reverse=True)

    # 1. JSON ì €ì¥
    final_data = {
        "last_updated": datetime.now().strftime("%Y.%m.%d"),
        "shrines": shrines
    }
    with open(JSON_OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    # 2. Sitemap ì €ì¥
    sitemap_content = generate_sitemap(shrines)
    with open(SITEMAP_OUTPUT, 'w', encoding='utf-8') as f:
        f.write(sitemap_content)
        
    # 3. Ads.txt ì €ì¥
    ads_content = "google.com, pub-8780435268193938, DIRECT, f08c47fec0942fa0"
    with open(ADS_OUTPUT, 'w', encoding='utf-8') as f:
        f.write(ads_content)

    print(f"\nğŸ‰ ë¹Œë“œ ì™„ë£Œ! ì´ {len(shrines)}ê°œ ì‹ ì‚¬ ë°ì´í„° ì²˜ë¦¬ë¨.")

if __name__ == "__main__":
    main()