# hatena_client.py
import os
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import base64
import hashlib
from datetime import datetime, timezone
import random
import re

# í™˜ê²½ ë³€ìˆ˜
HATENA_USERNAME = os.getenv('HATENA_USERNAME')
HATENA_BLOG_ID = os.getenv('HATENA_BLOG_ID')
HATENA_API_KEY = os.getenv('HATENA_API_KEY')

def create_wsse_header(username, api_key):
    nonce = hashlib.sha1(str(random.random()).encode()).digest()
    created = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    digest_base = nonce + created.encode() + api_key.encode()
    digest = hashlib.sha1(digest_base).digest()
    return f'UsernameToken Username="{username}", PasswordDigest="{base64.b64encode(digest).decode()}", Nonce="{base64.b64encode(nonce).decode()}", Created="{created}"'

def get_all_posts():
    print("ğŸ” í•˜í…Œë‚˜ ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    posts = []
    url = f"https://blog.hatena.ne.jp/{HATENA_USERNAME}/{HATENA_BLOG_ID}/atom/entry"
    
    max_pages = 20 
    current_page = 0
    
    # ê¸°ë³¸ ì¸ë„¤ì¼ ê²½ë¡œ
    DEFAULT_THUMBNAIL = "/static/images/JinjaMapLogo_Horizontal.png"

    while url and current_page < max_pages:
        headers = {'X-WSSE': create_wsse_header(HATENA_USERNAME, HATENA_API_KEY)}
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        try:
            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom', 'app': 'http://www.w3.org/2007/app'}
        except ET.ParseError:
            break
        
        entries = root.findall('atom:entry', ns)
        
        for entry in entries:
            # 1. ë¹„ê³µê°œ ê¸€ ì œì™¸
            control = entry.find('app:control', ns)
            if control is not None:
                draft = control.find('app:draft', ns)
                if draft is not None and draft.text == 'yes':
                    continue

            # 2. ê¸°ë³¸ ì •ë³´
            title_tag = entry.find('atom:title', ns)
            title = title_tag.text if title_tag is not None else "No Title"
            
            link_tag = entry.find('atom:link[@rel="alternate"]', ns)
            link = link_tag.get('href') if link_tag is not None else ""
            
            categories = [cat.get('term') for cat in entry.findall('atom:category', ns)]
            
            published_tag = entry.find('atom:published', ns)
            published_date = published_tag.text[:10] if published_tag is not None else ""

            content_tag = entry.find('atom:content', ns)
            content_html = content_tag.text if content_tag is not None else ""
            
            soup = BeautifulSoup(content_html, 'html.parser')
            content_text = soup.get_text(separator=" ")

            # 3. [ì´ë¯¸ì§€ ì¶”ì¶œ ë¡œì§ ê°•í™”]
            thumbnail = DEFAULT_THUMBNAIL
            
            # (1ë‹¨ê³„) HTML <img> íƒœê·¸ ê²€ìƒ‰
            images = soup.find_all('img')
            for img in images:
                src = img.get('src')
                if src and "f.st-hatena.com" in src and "icon" not in src:
                    thumbnail = src
                    break
            
            # (2ë‹¨ê³„) <img>ê°€ ì—†ë‹¤ë©´ í•˜í…Œë‚˜ ë¬¸ë²• [f:id:...] íŒŒì‹± (ìŠ¤í¬ë¦°ìƒ· ë¬¸ì œ í•´ê²°ìš©)
            # íŒ¨í„´: [f:id:ì•„ì´ë””:ì‹œê°„:...]
            if thumbnail == DEFAULT_THUMBNAIL:
                # ì •ê·œì‹ìœ¼ë¡œ id, timestamp, í™•ì¥ìì½”ë“œë¥¼ ì°¾ìŒ
                # ì˜ˆ: [f:id:starful:20251123173439p:plain] -> p=png
                match = re.search(r'\[f:id:([^:]+):([0-9]{14})([a-z])?:.*?\]', content_text)
                if match:
                    h_user = match.group(1) # starful
                    h_time = match.group(2) # 20251123173439
                    h_type = match.group(3) # p, j, g ë“±
                    
                    h_date = h_time[:8] # 20251123
                    
                    # í™•ì¥ì ì¶”ë¡  (p=png, j=jpg, g=gif, ì—†ìœ¼ë©´ jpg)
                    ext = 'png' if h_type == 'p' else 'gif' if h_type == 'g' else 'jpg'
                    
                    # í•˜í…Œë‚˜ ì´ë¯¸ì§€ ì„œë²„ URL êµ¬ì„±
                    thumbnail = f"https://cdn-ak.f.st-hatena.com/images/fotolife/{h_user[0]}/{h_user}/{h_date}/{h_time}.{ext}"

            # 4. [ë³¸ë¬¸ ìš”ì•½ ì •ë¦¬] (ìŠ¤í¬ë¦°ìƒ·ì˜ [f:id...] í…ìŠ¤íŠ¸ ì œê±°)
            # í•˜í…Œë‚˜ ë¬¸ë²• íƒœê·¸ ì œê±°
            clean_summary = re.sub(r'\[f:id:[^\]]+\]', '', content_text)
            # ê³µë°± ì •ë¦¬ ë° ê¸¸ì´ ì œí•œ
            clean_summary = re.sub(r'\s+', ' ', clean_summary).strip()
            summary = clean_summary[:180] + "..." if len(clean_summary) > 180 else clean_summary

            # 5. [ì£¼ì†Œ ì¶”ì¶œ]
            address = None
            addr_match = re.search(r'(ì†Œì¬ì§€|ì£¼ì†Œ|ìœ„ì¹˜|Address)\s*[:ï¼š]?\s*([^\n\r]+)', content_text)
            if addr_match:
                candidate = addr_match.group(2).strip()
                if len(candidate) < 50 and ('ã€’' in candidate or any(x in candidate for x in ['ë„', 'ì‹œ', 'êµ¬', 'í˜„', 'ç”º', 'çœŒ', 'å¸‚', 'åŒº'])):
                    address = candidate
            
            if not address:
                clean_title = re.sub(r'\[.*?\]', '', title)
                clean_title = re.split(r'[:ï¼š|\-â€“~]', clean_title)[0].strip()
                clean_title = re.sub(r'\(.*?\)', '', clean_title).strip()
                clean_title = clean_title.replace("ë¥¼ ì°¾ì•„ì„œ", "").replace("ë°©ë¬¸", "").replace("ì—¬í–‰", "").replace("í›„ê¸°", "").strip()
                
                if 1 < len(clean_title) < 30:
                    address = clean_title
                else:
                    continue

            posts.append({
                "title": title,
                "link": link,
                "published": published_date,
                "categories": categories,
                "thumbnail": thumbnail, 
                "address": address, 
                "summary": summary
            })

        next_link = root.find('atom:link[@rel="next"]', ns)
        url = next_link.get('href') if next_link is not None else None
        current_page += 1
        
    return posts