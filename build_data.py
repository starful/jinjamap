import os
import json
import frontmatter
import markdown
from bs4 import BeautifulSoup
from datetime import datetime

# ì„¤ì •
CONTENT_DIR = 'app/content'
JSON_OUTPUT = 'app/static/json/shrines_data.json'
SITEMAP_OUTPUT = 'app/static/sitemap.xml'
BASE_URL = 'https://jinjamap.com'

def strip_markdown(text):
    """ë§ˆí¬ë‹¤ìš´ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìš”ì•½ë¬¸ ìƒì„±ìš©)"""
    try:
        html = markdown.markdown(text)
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text()
    except Exception as e:
        print(f"Warning: Text strip failed - {e}")
        return text

def generate_sitemap(shrines):
    """ì‚¬ì´íŠ¸ë§µ XML ìƒì„±"""
    xml = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    xml += '  <url>\n'
    xml += f'    <loc>{BASE_URL}/</loc>\n'
    xml += f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>\n'
    xml += '    <changefreq>daily</changefreq>\n'
    xml += '    <priority>1.0</priority>\n'
    xml += '  </url>\n'

    for shrine in shrines:
        link = shrine['link']
        date_str = shrine['published']
        
        xml += '  <url>\n'
        xml += f'    <loc>{BASE_URL}{link}</loc>\n'
        xml += f'    <lastmod>{date_str}</lastmod>\n'
        xml += '    <changefreq>weekly</changefreq>\n'
        xml += '    <priority>0.8</priority>\n'
        xml += '  </url>\n'
        
    xml += '</urlset>'
    return xml

def main():
    print("ğŸ”¨ ë¡œì»¬ ë§ˆí¬ë‹¤ìš´ ë°ì´í„° ë¹Œë“œ ì‹œì‘...")
    
    shrines = []
    
    os.makedirs(os.path.dirname(JSON_OUTPUT), exist_ok=True)
    os.makedirs(os.path.dirname(SITEMAP_OUTPUT), exist_ok=True)

    if not os.path.exists(CONTENT_DIR):
        os.makedirs(CONTENT_DIR)

    for filename in os.listdir(CONTENT_DIR):
        if not filename.endswith('.md'):
            continue
            
        filepath = os.path.join(CONTENT_DIR, filename)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
                
                # Draft ê¸°ëŠ¥ (ê°œë°œí™˜ê²½ ë³€ìˆ˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ)
                if post.get('draft') == True and not os.environ.get('DEV_MODE'):
                    continue

                if not post.get('lat') or not post.get('lng'):
                    continue

                date_val = post.get('date')
                if date_val:
                    published_date = str(date_val)
                else:
                    published_date = datetime.now().strftime('%Y-%m-%d')

                # ìš”ì•½ë¬¸ ìƒì„±
                summary = post.get('summary')
                if not summary:
                    clean_text = strip_markdown(post.content)
                    summary = clean_text[:120] + '...'
                
                # [í•µì‹¬] ì˜¨ì²œ ì •ë³´ ìœ ë¬´ í™•ì¸
                content_str = str(post.content)
                has_onsen = "Relax at a Nearby Onsen" in content_str or "Nearby Attractions: Hot Springs" in content_str

                shrine = {
                    "id": filename.replace('.md', ''),
                    "title": post.get('title', 'No Title'),
                    "lat": post.get('lat'),
                    "lng": post.get('lng'),
                    "categories": post.get('categories', []),
                    "thumbnail": post.get('thumbnail', '/static/images/default.png'),
                    "address": post.get('address', ''),
                    "published": published_date,
                    "summary": summary,
                    "link": f"/shrine/{filename.replace('.md', '')}",
                    "has_onsen": has_onsen # ğŸ‘ˆ JSON í•„ë“œ ì¶”ê°€
                }
                shrines.append(shrine)

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ ({filename}): {e}")

    # ìµœì‹ ìˆœ ì •ë ¬
    shrines.sort(key=lambda x: x['published'], reverse=True)

    final_data = {
        "last_updated": datetime.now().strftime("%Y.%m.%d"),
        "shrines": shrines
    }
    with open(JSON_OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    sitemap_content = generate_sitemap(shrines)
    with open(SITEMAP_OUTPUT, 'w', encoding='utf-8') as f:
        f.write(sitemap_content)

    print(f"\nğŸ‰ ë¹Œë“œ ì™„ë£Œ! ì´ {len(shrines)}ê°œ")

if __name__ == "__main__":
    main()